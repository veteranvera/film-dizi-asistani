# app.py
# Streamlit arayÃ¼zÃ¼ - kullanÄ±cÄ±dan soru alÄ±r, retriever ile ilgili belgeleri bulur ve
# PromptNode aracÄ±lÄ±ÄŸÄ±yla LLM'den yanÄ±t dÃ¶ndÃ¼rÃ¼r.

import streamlit as st
from config import get_embedding_model, DATA_PATH, APP_TITLE
from data_processing import load_documents
from vector_store_manager import create_vector_store
from rag_pipeline import create_rag_pipeline

st.set_page_config(page_title=APP_TITLE, page_icon="ğŸ¥", layout="centered")
st.title(APP_TITLE)
st.markdown("""Bu kÃ¼Ã§Ã¼k uygulama, Ã¶nceden yÃ¼klenmiÅŸ film/dizi bilgileri Ã¼zerinden 
            sorulara kaynaklÄ± cevaplar Ã¼retir. """)

# YÃ¼kleme dÃ¼ÄŸmeleri / durum gÃ¶stergesi
if 'initialized' not in st.session_state:
    st.session_state['initialized'] = False

if not st.session_state['initialized']:
    with st.spinner('Belgeler yÃ¼kleniyor ve embedding oluÅŸturuluyor (ilk seferde zaman alabilir)...'):
        # Belgeleri yÃ¼kle
        docs = load_documents(DATA_PATH)

        # Embedding modelini al
        embedding_model = get_embedding_model()

        # VektÃ¶r deposu ve retriever oluÅŸtur
        document_store, retriever = create_vector_store(docs, embedding_model)

        # RAG pipeline oluÅŸtur (LLM ayarlarÄ±nÄ± .env veya ortam deÄŸiÅŸkenlerinden almayÄ± tercih edin)
        pipeline = create_rag_pipeline(retriever, llm_model_name=None, api_key=None)

        # Store in session state
        st.session_state['docs'] = docs
        st.session_state['document_store'] = document_store
        st.session_state['retriever'] = retriever
        st.session_state['pipeline'] = pipeline
        st.session_state['initialized'] = True
    st.success('HazÄ±r âœ…')

query = st.text_input('Soru yazÄ±n:', placeholder='Ã–rnek: "MCU izleme sÄ±rasÄ± nasÄ±l?"')

if st.button('Sor'):
    if not query.strip():
        st.warning('LÃ¼tfen bir soru yazÄ±n.')
    else:
        with st.spinner('YanÄ±t hazÄ±rlanÄ±yor...'):
            pipeline = st.session_state['pipeline']
            output = pipeline.run(query=query, params={"Retriever": {"top_k": 5}})
            # PromptNode cevabÄ±nÄ± al
            answers = output.get('answers') or output.get('output') or output.get('results')
            st.markdown('### ğŸ“ YanÄ±t')
            # Basit gÃ¶sterim (pipeline Ã§Ä±ktÄ±sÄ±na gÃ¶re uyarlanmÄ±ÅŸtÄ±r)
            st.write(output)
